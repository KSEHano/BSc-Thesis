\section{State of the research}

Automatic translation is a useful tool. The main motivations are economical in terms of time and money. BLEU is still widely used but beside that many other metrics have emerged. Their goal is to improve on BLEU and give results that better correlate with human judgment. In the following I will present some of the problems that BLEU has and some other metrics and how they work.



BLEU is a old score, especially in the fast changing field of computer science and mainly used because it is fast and easy to use. This ease of use and speed however comes at the price of its quality. BLEU has several flaws that are not addressed by the algorithm itself but since its publication several other systems have come forth improving automatic evaluation. Later I will present four famous metrics. However, I will first present the problems BLEU has that are the reasons improvements are necessary to have a better correlation with human judgment.



The first problem is that the scores for single sentences are not very accurate [@METEOR]. This is certainly a problem I had as well. The BLEU score is calculated via a geometric mean that leads to the problem that often scores are 0 if one of the n-grams are 0. This problem can be avoided by changing the weights as I have done and will explain in the third section. The problem is than that the weights can not be freely manipulate to have a good correlation with human judgment but so that the score is not always 0 if the segment is to short to have 4-grams. A more accurate score for single segments would would be useful in the fine-tuning of MT systems [@METEOR]. Automatic translation evaluation could therefor profit from a more accurate sentence score.



Another problem is that BLEU needs exact matches. It is even case sensitive, I describe below how I handled this difficulty. It does not consider synonyms and words with the same stem the same. That is a problem as adequate alternative translations in terms of synonyms are not considered the same [@METEOR]. There are infinitely many correct translation not recognising alternative translations leads to the fact that there are many correct translations are not recognised as such [@TERp p.118]. The problem is reduced with several reference translations but even those are only a small subset of all correct translations . @recall even suggest that stemming reference and hypothesis already improves the correlation to human judgment.  A metric could profit by matching synonyms and different word forms even if that has higher computational cost and an additional resources are needed in that process.



Another point that is not considered in BLEU is recall. Unigram recall is the number of matches divided by the number of words in the *reference* [@recall]. The main calculation in BLEU is the so called precision which is the number of matched n-grams divided by the total number of fords in the *hypothesis* [@BLEU]. The reason for that is that recall is used on one singular reference. So the best match of one reference is used while BLEU can combine the matches of several references and combine those to calculate the scores. To avoid a problem with the length of the translations BLEU include a brevity penalty, however, the results in @recall suggest that this is not enough to get results with good correlation without considering recall. Considering the study, recall seems to be a powerful tool achieve good correlation with human judgment and should be a part of a metric and heavily weighted. Recall could be a tool to improve correlation but it must be considered how to include it if one wants to pick matches from several translations at the same time and not only from the best match.



Considering the problems with BLEU, several alternative automatic evaluation methods have emerged. They have some similarities to BLEU but changes have been made to improve the correlation between the human and automatic judgment. Here I include four of the most well known metrics that are alternatives to BLEU. NIST is the closest to BLEU, METEOR is also very similar. All of them calculate how similar segments are to each other by checking if the words and n-grams are the same. TER and its variants are a little different in that they calculate the editing rate between a reference and the hypothesis.



@NIST have developed the evaluation method that resembles BLEU the most, it is called NIST. Like all the automatic evaluation methods NIST requires at least one reference translation. Further it compares the different n-grams of the reference(s) to the hypothesis and gives the whole segment a score. NIST improves on BLEU by including information weights, i. e. that n-grams with a lower chance of co-occurrence are weighted more heavily than those with a high chance of co-occurrence [@NIST]. With those improvements NIST has a slightly higher correlation with human judgment in terms of recognising adequacy but not fluency [@NIST]. NIST is very similar to BLEU and improves slightly on it in terms of correlation with human judgment.



The next evaluation metric I want to introduce is METEOR. Again METEOR relies on a human made reference translation to score the MT. One of the big problems in BLEU is that there words have to be the *exact* same to be considered a match, a mentioned above. METEOR works around this problem. It can match synonyms and word with the help of a word net [@METEOR]. This helps to give a more accurate score as it better considers alternative translations as adequate. Another change in comparison to BLEU is that METEOR supports recall and has three tuneable parameters[@METEOR]. The three parameter can be tuned to better correlate with human judgment. The first parameter controls the weights for precision and recall, the second shape of the penalty function and the third the weights of the penalty [@METEOR]. These parameters help to tune the metric to the system it is evaluating and give so a scores that better correlate with human judgment. METEOR changes BLEU by considering synonyms and word stems, including recall and having three tuneable parameters to get a better correlation with human judgment.



Other than the the previous two the TER score does not measure the how different a translation is from a reference translation, instead it measures how many edits have to be preformed to change a MT into the closest human reference [@TER]. Hence the name *translation edit rate* or TER. Other than the other metrics it counts the number of edits that have to be done to get from the translation to a reference and not the matches between hypothesis and reference. Edits include deletions, insertions, substitutions and shifts. To reduce the processing time to find the minimal number of edits for a given segment the algorithm first calculates the shift, that reduces the number of edits the most with a greedy search algorithm and then the other edits [@TER]. This is the basic algorithm but there are two versions of it.   
First there is the so called *human-targeted translation edit rate* or HTER. Additionally to the references of human translations there is a human targeted reference. This means that human annotator, that are fluent in the target language are given the MT and the references and then edit the MT reference with minimal edits to be a fluent and adequate translation [@TER]. The edited MT translation is then used as another reference to calculate the score. This can be used when working on a MT system as the translation do not change that much with every change to the system. If they do change the annotators can work on the segments that changed again. The most pressing problem here is that the annotator have to work on each sentence. This is time and cost consuming as they need 3 to 7 minutes per sentence [@TER]. So while HTER is an improvement on TER in terms of correlation between human and automatic evaluation it is quite work intensive.    
The second variation on TER is TER-Plus or TERp. It works like TER with at least one reference and then calculated the optimal edits. Like METEOR it considers recall and with help of a word net synonyms and stems are considered the same words if they appear in the translation. Other than the basic TER algorithm it also does have different costs for edits, i. e. shifts, substitutions, deletions and insertions do not have all the same cost of 1 they have in TER [@TERp]. These parameters for cost are optimized to correlate with human judgment on a specific system with the help of a hill-climbing algorithm. The cost can be even further specified to certain word groups so that TERp can get an even higher correlation with human judgment[@TERp].    
TER is a little bit different from the other metrics in that it calculates the edit rate and not the similarities between hypothesis and reference. The variations of TER, HTER and TERp, try to improve on TER to give a more accurate editing cost. They are both more costly than TER because they rely on more resources. HTER is especially costly as it requires human labour. TER and its variants are closer to human evaluation but a little more costly than BLEU.