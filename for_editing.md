\section{State of the Research}

Automatic translation is a useful tool that is used to evaluate the quality of translations fast and cheap. BLEU is still widely used but beside that many other metrics have emerged. The goal is to create a metric which gives results that correlate better with human judgment than BLEU. In the following I will present some of the problems that BLEU has and some other metrics and how they work.



BLEU is an old metric, especially in the fast changing field of computer science. It is still relevant because it is fast and easy to use. This ease of use and speed however comes at the price of its quality. BLEU has several flaws that other publications try to fix in their own metrics. The problems that appear in BLEU are inaccurate scores for single sentences or segments, a reliance on exact word matches and no recall.



The first problem of BLEU that other metrics try to improve are the inaccurate scores for single sentences [@METEOR]. This is certainly a problem I had as well. The BLEU score is calculated via a geometric mean that leads to the problem that often scores are 0 if one of the n-grams are 0. This problem can be avoided by changing the weights as I have done and will explain in the third section. The problem is than that the weights can not be freely manipulate to have a good correlation with human judgment. The score should not be so strongly influenced by the absence of a n-gram. A more accurate score for single segments would be useful in the fine-tuning of MT systems [@METEOR]. Therefore automatic translation evaluation could profit from a more accurate sentence score. Solving the following problems can also help with the accuracy of the score for single sentences.



One of the other problems is that BLEU needs exact matches. BLEU is even case sensitive, I describe in part three how I handled this difficulty. The BLEU metric also does not consider synonyms and words with the same stem as a match. That is a problem as adequate alternative translations in terms of synonyms are not considered to be good translations [@METEOR]. There are infinitely many correct translations. Not recognising alternative translations leads to the fact that there many correct translations are not recognised as such [@TERp p.118]. To alleviate this problem BLEU can use several different translations but even with those only small subset of all possible good translations can be considered by BLEU. @recall even suggest that stemming reference and hypothesis already improves the accuracy of a metric.  A metric could profit by matching synonyms and different word forms even if that has higher computational cost for matching words and an additional resources are needed in that process.



Another point that is not considered in BLEU is recall. Recall is the number of matches divided by the number of words or n-grams in the *reference* [@recall]. The main calculation in BLEU is the so called *precision* which is the number of matched n-grams divided by the total number of fords in the *hypothesis* [@BLEU]. The reason BLEU uses precision instead of recall is that recall has to work with the reference which matches the hypothesis the best. On the other hand, BLEU can combine the matches of several references and combine those to calculate the scores. BLEU includes a brevity penalty to avoid that shorter translations are always preferred. However, the results in @recall suggest that the penalty is not enough to get results with good correlation with human judgment. Considering the study, recall seems to be a powerful tool  to achieve good correlation with human judgment and should be a part of a metric and heavily weighted. Recall could be a tool to improve correlation but it must be considered how to include it if one wants to pick matches from several references to further improve the accuracy of the scores.



Considering the problems with BLEU, several alternative metrics have emerged to tackle the problems of BLEU. Here I include three of the most well known metrics that are alternatives to BLEU namely NIST, METEOR and TER including the two variants of Ter HTER and TER-Plus. NIST is the most similar to BLEU, METEOR includes several measures to improve on BLEU and TER is the most distinct of the three.



@NIST have developed the evaluation metric that resembles BLEU the most, it is called NIST. Like all the automatic evaluation methods NIST requires at least one reference translation. Further it compares the different n-grams of the reference or references to the hypothesis and gives the whole segment a score. NIST improves on BLEU by including information weights, i. e. that n-grams with a lower chance of co-occurrence are weighted more heavily than those with a high chance of co-occurrence [@NIST]. With those improvements NIST has a slightly higher correlation with human judgment in terms of recognising adequacy but not fluency [@NIST]. NIST is very similar to BLEU and improves slightly on it in terms of correlation with human judgment.



The next evaluation metric I want to introduce is METEOR. METEOR relies on a human made reference translation to score the MT like BLEU and NIST. One of the big problems in BLEU is that there words have to be *exactly* the same to be considered a match, as mentioned above. METEOR works around this problem, it can match synonyms as well as word stems with the help of a word net [@METEOR]. This helps to give a more accurate score as it better considers alternative translations as adequate. Another change in comparison to BLEU is that METEOR supports recall and has three tuneable parameters [@METEOR]. The three parameters can be tuned to better correlate with human judgment depending on the MT system given. The first parameter controls the weights for precision and recall, the second the shape of the penalty function and the third the weights of the penalty [@METEOR]. These parameters help to tune the metric to the system it is evaluating and give a scores that better correlates with human judgment. METEOR changes BLEU by considering synonyms and word stems, including recall and having three tuneable parameters to get a better correlation with human judgment.



Other than the the previous two the TER score does not measure the how different a translation is from a reference translation, instead it measures how many edits have to be performed to change a MT into the closest human reference [@TER]. Hence the name *translation edit rate* or TER. Edits include deletions, insertions, substitutions and shifts. To reduce the processing time to find the minimal number of edits for a given segment the algorithm first calculates the shift, that reduces the number of edits the most with a greedy search algorithm and then the other edits [@TER]. This is the basic algorithm but there are two versions of it.   
The first variant is the *human-targeted translation edit rate* or HTER. Additionally to the references of human translations there is a human targeted reference. This means that human annotator, that are fluent in the target language are given the MT and the references. Then edit the MT reference with minimal edits to be a fluent and adequate translation [@TER]. The edited MT translation is then used as another reference to calculate the score. This can be used when working on a MT system as the translation do not change that much with every change in the MT system. If they do change, the annotators can work on the segments that changed again. The most pressing problem here is that the annotators have to work on each sentence. This is time and cost consuming as they need 3 to 7 minutes per sentence [@TER]. So while HTER is an improvement on TER in terms of correlation between human and automatic evaluation it is quite work intensive and expensive.    
The second variation of TER is TER-Plus or TERp. It works like TER with at least one reference and then calculated the optimal edits. Like METEOR it considers recall. Additionally, like METEOR, synonyms and word stems are considered in the calculation of the editing rate. Other than the basic TER algorithm it also has different costs for edits, i. e. shifts, substitutions, deletions and insertions do not have all the same cost of 1 as they have in TER [@TERp]. These parameters for cost are optimized to correlate with human judgment on a specific system with the help of a hill-climbing algorithm. The cost can be even further specified to certain word groups so that TERp can get an even higher correlation with human judgment[@TERp].    
TER is a little bit different from the other metrics in that it calculates the edit rate and not the similarities or matches between hypothesis and reference. The variations of TER, HTER and TERp, try to improve on TER to give a more accurate editing cost. They are both more costly than TER because they rely on more resources. HTER is especially costly as it requires human labour. TER and its variants are closer to human evaluation but a little more costly than BLEU.



Despite the development of arguably better metrics, BLEU is still in use. For one every metric compares itself at least to BLEU to how how they improve the score. All the metrics I mentioned above prove there merit by comparing themselves to BLEU. When studying metrics BLEU is still used as a baseline. Another use of BLEU is in studies that test the quality of MT systems. Here BLEU is used because it does not require many resources and is language independent. @Post used BLEU to compare a customized statistical MT (SMT) system with a neural MT (NMT) system Microsoft translator Hub (MTH) and DeepL respectively. They evaluated the results with BLEU and after that compared that to human evaluation. They found out that the NMT is better than the SMT but that while BLEU judges the NMT to be better it also underestimates the NMT.    
Similarly another study examined the quality of Google Translate [@Legal]. They translated legal text from English to Croatian and evaluate the quality with BLEU with up to three references. Like the study above they confirm their results with a human evaluation. They come to the conclusion that BLEU correlates better with human judgment if it has more references. Human evaluation while more time consuming is still valued as more accurate and despite the high subjectivity has a good inter judge correlation.   
Those are only two studies that use BLEU as late as 2018. BLEU is in fact used very frequently a feat considering it was introduced in 2002. Sadly the reporting of BLEU scores seems to be inconsistent as @Clarity criticizes. They present the problem that paper which use BLEU do not report how they may changed BLEU when they used it. This is problematic as it does not allow for a comparison between different papers. Depending on the papers it is unclear if a certain MT system is better than the other if it has different scores in different papers. They suggest to improve reports of any changes and use BLEU only to compare similar systems. This brings me to the conclusion that BLEU is still a very well used tool despite its difficulties. 



All in all BLEU is not the best metric available. Several aspects of the metric have room for improvement to get a score closer to human evaluations. There are several metrics that have a better correlation to human evaluation. I mentioned NIST, METEOR and TER as some of the well known alternatives. NIST is not much of an improvement compared to BLEU and METEOR as well as TER need more resources than BLEU. In the end BLEU is still a metric that is often used to compare to other metrics as a baseline, or to evaluate MT systems.