@inproceedings{BLEU,
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation , and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1},
annote = {mashine evaluation
BLEU translation metric easy with at least one big reference by several translators for diferent styles
matching candidate n-gramms agaist translation
n-gramm in hypo/ n-gramm in reference
brevety panality
reference one is okay if from different translators},
author = {{Papineni, K., Roukos, S., Ward, T., & Zhu}, W. J.},
booktitle = {Proceedings of the 40th annual meeting of the Association for Computational Linguistic},
doi = {10.1002/andp.19223712302},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Papineni, K., Roukos, S., Ward, T., & Zhu - 2002 - BLEU a Method for Automatic Evaluation of Machine Translation.pdf:pdf},
issn = {15213889},
mendeley-groups = {Thesis2},
pages = {311--318},
title = {{BLEU: a Method for Automatic Evaluation of Machine Translation}},
year = {2002}
}

@book{book,
abstract = {This is the first volume that brings together research and practice from academic and industry settings and a combination of human and machine translation evaluation. Its comprehensive collection of papers by leading experts in human and machine translation quality and evaluation who situate current developments and chart future trends fills a clear gap in the literature. This is critical to the successful integration of translation technologies in the industry today, where the lines between human and machine are becoming increasingly blurred by technology: this affects the whole translation landscape, from students and trainers to project managers and professionals, including in-house and freelance translators, as well as, of course, translation scholars and researchers. The editors have broad experience in translation quality evaluation research, including investigations into professional practice with qualitative and quantitative studies, and the contributors are leading experts in their respective fields, providing a unique set of complementary perspectives on human and machine translation quality and evaluation, combining theoretical and applied approaches.},
annote = {Part I: 1 p.20-43 Introduction
Part II und III generell sehr interesant
Part III Specia: several sources to look at for Quest},
author = {Moorkens, Joss; and Sheila;, Castilho and Gaspari, Federico; and Doherty, Stephen},
booktitle = {Machine Translation},
doi = {10.1007/s10590-019-09241-w},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moorkens et al. - 2018 - Translation quality assessment from principles to practice.pdf:pdf},
issn = {0922-6567},
mendeley-groups = {Thesis,Thesis2},
title = {{Translation quality assessment: from principles to practice}},
url = {http://www.springer.com/series/15798},
year = {2018}
}
@inproceedings{Re-evaluating,
abstract = {We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality , and give two significant counterexamples to Bleu's correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.},
annote = {Bleu as an automatic measure is not the be all end all as it allows for translation to be scored similarly when they are different in quality if a human scores them

translations of similar systems?},
author = {{Callison-Burch, Chris; OSBORNE, Miles; KOEHN}, Philipp},
booktitle = {11th Conference of the European Chapter of the Association for Computational Linguistics},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Callison-Burch, Chris OSBORNE, Miles KOEHN - 2006 - Re-evaluating the Role of BLEU in Machine Translation Research.pdf:pdf},
issn = {0037-7732},
keywords = {Adolescent Attitudes,Adolescent Behavior,Adolescents,Expectation,Family Influence,Peer Influence,Predictor Variables,Secondary School Students,Sexuality,Social Environment,Student Attitudes,Violence},
mendeley-groups = {Thesis2},
title = {{Re-evaluating the Role of BLEU in Machine Translation Research}},
year = {2006}
}
@article{SentenceLength,
abstract = {The cognitive load theory recommendations for enhancing the success of teaching are effective up to a certain boundary. The paper is dedicated to finding this boundary line in sentence length for 17-18-year-old students. The students filled in the blanks in 30 cloze tests. The cloze test results were correlated with the percentage of sentences over the boundary line. When the boundary-line was low, the coefficient of correlation increased with the rising of the line and the coefficient began to drop when the boundary line passed 140 characters. This size of the boundary line indicated the sentence length, up to which the taking of the load from the learners' mind was effective. The sentences with 130-50 characters were the most suitable for the students.},
annote = {most counts in characters but understanding gets worst in 130-150 characters which are 15-17 words -> use 15 words as upper boundary

good readers can process more but i make lower bounds for it to be easier},
author = {Mikk, Jaan},
doi = {10.1080/03055690701811164},
file = {:C\:/Users/Allgemein/Bachelor/7.Semester/Thesis/paper/Mikk_2008_sentence_length.pdf:pdf},
issn = {03055698},
journal = {Educational Studies},
keywords = {Cloze test,Cognitive load theory,Sentence comprehension,Textbooks,Upper secondary school},
mendeley-groups = {Thesis2},
number = {2},
pages = {119--127},
title = {{Sentence length for revealing the cognitive load reversal effect in text comprehension}},
volume = {34},
year = {2008}
}
@inproceedings{BetterTrans,
abstract = {Many machine translation evaluation metrics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment. It has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation. However, to date there has been no unambiguous report that these new metrics can improve a state-of-the-art machine translation system over its BLEU-tuned baseline. In this paper, we demonstrate that tuning Joshua, a hierarchical phrase-based statistical machine translation system, with the TESLA metrics results in significantly better human-judged translation quality than the BLEU-tuned baseline. TESLA-M in particular is simple and performs well in practice on large datasets. We release all our implementation under an open source license. It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems. {\textcopyright} 2011 Association for Computational Linguistics.},
annote = {comparison of metrics with machine translation and improvement of MT
mainly with training a MT engine
a lot of mre sources!
BLEU: simple, no training, no language specific reurces, tendency to favour short translation despite penality
TER: counts necessary edits for translation reference, simple, no language specific resources, similar to human inutuition
TESLA-M: matches bags of n-grams but with weighs, language spezific, very good for into english
TESLA-F: similar to tesla M, not so good out of english

Tesla outperforms other two consistently
teslas similar as well as Ter and Bleu},
author = {Liu, Chang and Dahlmeier, Daniel and Ng, Hwee Tou},
booktitle = {EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Dahlmeier, Ng - 2011 - Better evaluation metrics lead to better machine translation.pdf:pdf},
isbn = {1937284115},
mendeley-groups = {Thesis2},
pages = {375--384},
title = {{Better evaluation metrics lead to better machine translation}},
year = {2011}
}

@article{Clarity,
  title={A call for clarity in reporting BLEU scores},
  author={Post, Matt},
  journal={arXiv preprint arXiv:1804.08771},
  year={2018}
}

@article{TER,
abstract = {We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU-even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as-or better than-a second human judgment does. {\textcopyright} 2006 The Association for Machine Translation in the Americas.},
annote = {TER explained
hter:edit of the mt we look at. has to be changed if new trans is significantly different},
author = {Snover, Matthew and Dorr, Bonnie and Schwartz, Richard and Micciulla, Linnea and Makhoul, John},
file = {:C\:/Users/Allgemein/Bachelor/7.Semester/Thesis/paper/2006.amta-papers.25.pdf:pdf},
journal = {AMTA 2006 - Proceedings of the 7th Conference of the Association for Machine Translation of the Americas: Visions for the Future of Machine Translation},
mendeley-groups = {Thesis2},
number = {August},
pages = {223--231},
title = {{A study of translation edit rate with targeted human annotation}},
year = {2006}
}

@inproceedings{NIST,
  title={Automatic evaluation of machine translation quality using n-gram co-occurrence statistics},
  author={Doddington, George},
  booktitle={Proceedings of the second international conference on Human Language Technology Research},
  pages={138--145},
  year={2002}
}

@article{METEOR,
abstract = {The Meteor Automatic Metric for Machine Translation evaluation, originally developed and released in 2004, was designed with the explicit goal of producing sentence-level scores which correlate well with human judgments of translation quality. Several key design decisionswere incorporated into Meteor in support of this goal. In contrast with IBM's Bleu, which uses only precision-based features, Meteor uses and emphasizes recall in addition to precision, a property that has been confirmed by several metrics as being critical for high correlation with human judgments. Meteor also addresses the problem of reference translation variability by utilizing flexible word matching, allowing for morphological variants and synonyms to be taken into account as legitimate correspondences. Furthermore, the feature ingredients within Meteor are parameterized, allowing for the tuning of the metric's free parameters in search of values that result in optimal correlation with human judgments. Optimal parameters can be separately tuned for different types of human judgments and for different languages. We discuss the initial design of the Meteor metric, subsequent improvements, and performance in several independent evaluations in recent years. {\textcopyright} Springer Science+Business Media B.V. 2009.},
annote = {Meteor: references; lexical similarity; match synonyms and morphological variabnts},
author = {Lavie, Alon and Denkowski, Michael J.},
doi = {10.1007/s10590-009-9059-4},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavie, Denkowski - 2009 - The METEOR metric for automatic evaluation of Machine Translation.pdf:pdf},
issn = {09226567},
journal = {Machine Translation},
keywords = {Automatic metrics,MT evaluation,Machine Translation},
mendeley-groups = {Thesis2},
number = {2-3},
pages = {105--115},
title = {{The METEOR metric for automatic evaluation of Machine Translation}},
volume = {23},
year = {2009}
}

@article{TERp,
abstract = {This paper describes a new evaluation metric, TER-Plus (TERp) for automatic evaluation of machine translation (MT). TERp is an extension of Translation Edit Rate (TER). It builds on the success of TER as an evaluation metric and alignment tool and addresses several of its weaknesses through the use of paraphrases, stemming, synonyms, as well as edit costs that can be automatically optimized to correlate better with various types of human judgments. We present a correlation study comparing TERp to BLEU, METEOR and TER, and illustrate that TERp can better evaluate translation adequacy. {\textcopyright} Springer Science+Business Media B.V. 2009.},
annote = {Work on TER:
with reference (like ter)
considers synonyms and same stem
hes diferentiated cost for diffrent edits},
author = {Snover, Matthew G. and Madnani, Nitin and Dorr, Bonnie and Schwartz, Richard},
doi = {10.1007/s10590-009-9062-9},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Snover et al. - 2009 - TER-Plus Paraphrase, semantic, and alignment enhancements to translation Edit Rate.pdf:pdf},
isbn = {1059000990629},
issn = {09226567},
journal = {Machine Translation},
keywords = {Alignment,Machine translation evaluation,Paraphrasing},
mendeley-groups = {Thesis2},
number = {2-3},
pages = {117--127},
title = {{TER-Plus: Paraphrase, semantic, and alignment enhancements to translation Edit Rate}},
volume = {23},
year = {2009}
}

@article{TER,
abstract = {We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU-even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as-or better than-a second human judgment does. {\textcopyright} 2006 The Association for Machine Translation in the Americas.},
annote = {TER explained
hter:edit of the mt we look at. has to be changed if new trans is significantly different},
author = {Snover, Matthew and Dorr, Bonnie and Schwartz, Richard and Micciulla, Linnea and Makhoul, John},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Snover et al. - 2006 - A study of translation edit rate with targeted human annotation.pdf:pdf},
journal = {AMTA 2006 - Proceedings of the 7th Conference of the Association for Machine Translation of the Americas: Visions for the Future of Machine Translation},
mendeley-groups = {Thesis2},
number = {August},
pages = {223--231},
title = {{A study of translation edit rate with targeted human annotation}},
year = {2006}
}

@inproceedings{recall,
  title={The significance of recall in automatic metrics for MT evaluation},
  author={Lavie, Alon and Sagae, Kenji and Jayaraman, Shyamsundar},
  booktitle={Conference of the Association for Machine Translation in the Americas},
  pages={134--143},
  year={2004},
  organization={Springer}
}

@article{Post,
abstract = {Abstract This paper presents a study conducted in collaboration with Swiss Post's Language Service that aims to compare the performance of a generic neural machine translation system (DeepL) and a customised statistical machine translation system (Microsoft Translator Hub, MTH) in terms of post-editing effort and quality of the final translation for the language direction German-to-French. The results for automatic and human evaluations show that DeepL is overall better than MTH, but its quality is underestimated by the BLEU score.},
annote = {NMT (deepL) compared to SMT
das ist {\"{a}}hnlich zu dem was ich machen will, nur, dass ich die systme nicht selbs trainiere und die Systeme pre-editing von menschen vergleichen lasse},
author = {Volkart, Lise and Bouillon, Pierrette and Girletti, Sabrina},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Volkart, Bouillon, Girletti - 2018 - Statistical vs. Neural Machine Translation A Comparison of MTH and DeepL at Swiss Post's Language S.pdf:pdf},
journal = {Proceedings of the 40th Conference Translating and the Computer},
mendeley-groups = {Thesis2},
pages = {145--150},
title = {{Statistical vs. Neural Machine Translation: A Comparison of MTH and DeepL at Swiss Post's Language Service}},
url = {https://www.matecat.com},
year = {2018}
}

@article{legal,
abstract = {This paper presents work on the evaluation of online available machine translation (MT) service, i.e. Google Translate, for English-Croatian language pair in the domain of legislation. The total set of 200 sentences, for which three reference translations are provided, is divided into short and long sentences. Human evaluation is performed by native speakers, using the criteria of adequacy and fluency. For measuring the reliability of agreement among raters, Fleiss' kappa metric is used. Human evaluation is enriched by error analysis, in order to examine the influence of error types on fluency and adequacy, and to use it in further research. Translation errors are divided into several categories: non-translated words, word omissions, unnecessarily translated words, morphological errors, lexical errors, syntactic errors and incorrect punctuation. The automatic evaluation metric BLEU is calculated with regard to a single and multiple reference translations. System level Pearson's correlation between BLEU scores based on a single and multiple reference translations is given, as well as correlation between short and long sentences BLEU scores, and correlation between the criteria of fluency and adequacy and each error category.},
annote = {BLEU for translation evaluation

Bleu score is better with 2 or even 3 references},
author = {Seljan, Sanja and Vicic, Tomislav and Brkic, Marija},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seljan, Vicic, Brkic - 2012 - BLEU evaluation of machine-translated english-croatian legislation.pdf:pdf},
isbn = {9782951740877},
journal = {Proceedings of the 8th International Conference on Language Resources and Evaluation, LREC 2012},
keywords = {BLEU metric,English-Croatian legislation,Human evaluation},
mendeley-groups = {Thesis2},
pages = {2143--2148},
title = {{BLEU evaluation of machine-translated english-croatian legislation}},
year = {2012}
}
@misc{Google_NMT,
	title = {Zero-{Shot} {Translation} with {Google}’s {Multilingual} {Neural} {Machine} {Translation} {System}},
	url = {http://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html},
	abstract = {Posted by Mike Schuster (Google Brain Team), Melvin Johnson (Google Translate) and Nikhil Thorat (Google Brain Team) In the last 10 years, G...},
	language = {en},
	urldate = {2021-07-28},
	journal = {Google AI Blog},
	file = {Snapshot:C\:\\Users\\Allgemein\\Zotero\\storage\\JIG3DBGU\\zero-shot-translation-with-googles.html:text/html},
}

@misc{diffNMT,
	title = {Neural {Machine} {Translation} mit menschlichem {Touch}},
	url = {https://www.supertext.de/de/unternehmung/machine-translation},
	abstract = {Künstliche Intelligenz. Und menschliche Eloquenz. Was immer Sie brauchen, Supertext liefert die Lösung.},
	language = {de-DE},
	urldate = {2021-07-28},
	journal = {Supertext},
	file = {Snapshot:C\:\\Users\\Allgemein\\Zotero\\storage\\N75JXKE3\\machine-translation.html:text/html},
}le

@misc{SDL_NMT,
	title = {Neural {Machine} {Translation} in {Trados} {Studio}},
	url = {https://www.trados.com/products/machine-translation/access-nmt-in-studio.html},
	abstract = {Neural Machine Translation is the perfect solution for translators looking to use the latest in neural machine translation to automatically translate content.},
	language = {en},
	urldate = {2021-07-28},
	journal = {SDL},
	file = {Snapshot:C\:\\Users\\Allgemein\\Zotero\\storage\\N3EVSMLX\\access-nmt-in-studio.html:text/html},
}

@misc{DeepL,
	title = {{DeepL} {Translate} – {Der} präziseste Übersetzer der {Welt}},
	url = {https://www.DeepL.com/translator},
	abstract = {Schnell, präzise und sicher: Erfahren Sie, warum Millionen von Menschen täglich DeepL nutzen. Übersetzen Sie Texte und komplette Dateien im Handumdrehen. Derzeit werden die Sprachen Bulgarisch, Chinesisch, Dänisch, Deutsch, Niederländisch, Englisch, Estnisch, Finnisch, Französisch, Griechisch, Italienisch, Japanisch, Lettisch, Litauisch, Polnisch, Portugiesisch, Rumänisch, Russisch, Schwedisch, Slowakisch, Slowenisch, Spanisch, Tschechisch und Ungarisch unterstützt.},
	language = {de},
	urldate = {2021-07-28},
	file = {Snapshot:C\:\\Users\\Allgemein\\Zotero\\storage\\CJVIYWVJ\\translator.html:text/html},
}

@misc{Google,
	title = {Google Übersetzer},
	url = {https://translate.google.com/},
	urldate = {2021-07-28},
	file = {Google Übersetzer:C\:\\Users\\Allgemein\\Zotero\\storage\\5EMKS2QR\\translate.google.com.html:text/html},
}
@misc{Bing_NMT,
	title = {Translator {Text} {API} - {Microsoft} {Translator} for {Business}},
	url = {https://web.archive.org/web/20190902184414/https://www.microsoft.com/en-us/translator/business/translator-api/},
	urldate = {2021-07-28},
	month = sep,
	year = {2019},
}

@article{QE,
abstract = {Most evaluation metrics for machine translation (MT) require reference translations for each sentence in order to produce a score reflecting certain aspects of its quality. The de facto metrics, BLEU and NIST, are known to have good correlation with human evaluation at the corpus level, but this is not the case at the segment level. As an attempt to overcome these two limitations, we address the problem of evaluating the quality of MT as a prediction task, where reference-independent features are extracted from the input sentences and their translation, and a quality score is obtained based on models produced from training data.We showthat this approach yields better correlation with human evaluation as compared to commonly used metrics, even with models trained on different MT systems, language-pairs and text domains. {\textcopyright} Springer Science+Business Media B.V. 2010.},
annote = {reference independent evaluation (better for segment level)
QE might be better to compre different MT systems
this in the state of research},
author = {Specia, Lucia and Raj, Dhwaj and Turchi, Marco},
doi = {10.1007/s10590-010-9077-2},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Specia, Raj, Turchi - 2010 - Machine translation evaluation versus quality estimation.pdf:pdf},
issn = {09226567},
journal = {Machine Translation},
keywords = {Confidence estimation,Machine translation evaluation,Quality estimation},
mendeley-groups = {Thesis2},
month = {mar},
number = {1},
pages = {39--50},
title = {{Machine translation evaluation versus quality estimation}},
volume = {24},
year = {2010}
}

@article{Quest,
abstract = {We describe QUEST, an open source framework for machine translation quality estimation. The framework allows the ex- traction of several quality indicators from source segments, their translations, exter- nal resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efficacy of fea- tures and algorithms.},
annote = {framework for quality estimation
Quest website
Bl features 17 because good not significant improvement},
author = {Specia, Lucia and Shah, Kashif and {De Souza}, Jose G C and Cohn, Trevor and Kessler, Bruno},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Specia et al. - Unknown - QuEst-A translation quality estimation framework.pdf:pdf},
journal = {ACL (Conference System Demonstrations)},
keywords = {Artificial intelligence,Computer science,Language model,Machine learning,Machine translation,Natural language processing,Parsing,Topic model},
mendeley-groups = {Thesis},
pages = {79--84},
title = {{QuEst - A translation quality estimation framework}},
url = {http://www.dcs.shef.ac.uk/},
year = {2013}
}



