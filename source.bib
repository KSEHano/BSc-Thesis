@BLEU{PapineniK.RoukosS.WardT.&Zhu2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation , and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1},
annote = {mashine evaluation
BLEU translation metric easy with at least one big reference by several translators for diferent styles
matching candidate n-gramms agaist translation
n-gramm in hypo/ n-gramm in reference
brevety panality
reference one is okay if from different translators},
author = {{Papineni, K., Roukos, S., Ward, T., & Zhu}, W. J.},
booktitle = {Proceedings of the 40th annual meeting of the Association for Computational Linguistic},
doi = {10.1002/andp.19223712302},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Papineni, K., Roukos, S., Ward, T., & Zhu - 2002 - BLEU a Method for Automatic Evaluation of Machine Translation.pdf:pdf},
issn = {15213889},
mendeley-groups = {Thesis2},
pages = {311--318},
title = {{BLEU: a Method for Automatic Evaluation of Machine Translation}},
year = {2002}
}
@book{Moorkens2018,
abstract = {This is the first volume that brings together research and practice from academic and industry settings and a combination of human and machine translation evaluation. Its comprehensive collection of papers by leading experts in human and machine translation quality and evaluation who situate current developments and chart future trends fills a clear gap in the literature. This is critical to the successful integration of translation technologies in the industry today, where the lines between human and machine are becoming increasingly blurred by technology: this affects the whole translation landscape, from students and trainers to project managers and professionals, including in-house and freelance translators, as well as, of course, translation scholars and researchers. The editors have broad experience in translation quality evaluation research, including investigations into professional practice with qualitative and quantitative studies, and the contributors are leading experts in their respective fields, providing a unique set of complementary perspectives on human and machine translation quality and evaluation, combining theoretical and applied approaches.},
annote = {Part I: 1 p.20-43 Introduction
Part II und III generell sehr interesant
Part III Specia: several sources to look at for Quest},
author = {Moorkens, Joss; and Sheila;, Castilho and Gaspari, Federico; and Doherty, Stephen},
booktitle = {Machine Translation},
doi = {10.1007/s10590-019-09241-w},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moorkens et al. - 2018 - Translation quality assessment from principles to practice.pdf:pdf},
issn = {0922-6567},
mendeley-groups = {Thesis,Thesis2},
title = {{Translation quality assessment: from principles to practice}},
url = {http://www.springer.com/series/15798},
year = {2018}
}


