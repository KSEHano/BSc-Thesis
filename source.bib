@inproceedings{BLEU,
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation , and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1},
annote = {mashine evaluation
BLEU translation metric easy with at least one big reference by several translators for diferent styles
matching candidate n-gramms agaist translation
n-gramm in hypo/ n-gramm in reference
brevety panality
reference one is okay if from different translators},
author = {{Papineni, K., Roukos, S., Ward, T., & Zhu}, W. J.},
booktitle = {Proceedings of the 40th annual meeting of the Association for Computational Linguistic},
doi = {10.1002/andp.19223712302},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Papineni, K., Roukos, S., Ward, T., & Zhu - 2002 - BLEU a Method for Automatic Evaluation of Machine Translation.pdf:pdf},
issn = {15213889},
mendeley-groups = {Thesis2},
pages = {311--318},
title = {{BLEU: a Method for Automatic Evaluation of Machine Translation}},
year = {2002}
}

@book{book,
abstract = {This is the first volume that brings together research and practice from academic and industry settings and a combination of human and machine translation evaluation. Its comprehensive collection of papers by leading experts in human and machine translation quality and evaluation who situate current developments and chart future trends fills a clear gap in the literature. This is critical to the successful integration of translation technologies in the industry today, where the lines between human and machine are becoming increasingly blurred by technology: this affects the whole translation landscape, from students and trainers to project managers and professionals, including in-house and freelance translators, as well as, of course, translation scholars and researchers. The editors have broad experience in translation quality evaluation research, including investigations into professional practice with qualitative and quantitative studies, and the contributors are leading experts in their respective fields, providing a unique set of complementary perspectives on human and machine translation quality and evaluation, combining theoretical and applied approaches.},
annote = {Part I: 1 p.20-43 Introduction
Part II und III generell sehr interesant
Part III Specia: several sources to look at for Quest},
author = {Moorkens, Joss; and Sheila;, Castilho and Gaspari, Federico; and Doherty, Stephen},
booktitle = {Machine Translation},
doi = {10.1007/s10590-019-09241-w},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moorkens et al. - 2018 - Translation quality assessment from principles to practice.pdf:pdf},
issn = {0922-6567},
mendeley-groups = {Thesis,Thesis2},
title = {{Translation quality assessment: from principles to practice}},
url = {http://www.springer.com/series/15798},
year = {2018}
}
@inproceedings{Re-evaluating,
abstract = {We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality , and give two significant counterexamples to Bleu's correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.},
annote = {Bleu as an automatic measure is not the be all end all as it allows for translation to be scored similarly when they are different in quality if a human scores them

translations of similar systems?},
author = {{Callison-Burch, Chris; OSBORNE, Miles; KOEHN}, Philipp},
booktitle = {11th Conference of the European Chapter of the Association for Computational Linguistics},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Callison-Burch, Chris OSBORNE, Miles KOEHN - 2006 - Re-evaluating the Role of BLEU in Machine Translation Research.pdf:pdf},
issn = {0037-7732},
keywords = {Adolescent Attitudes,Adolescent Behavior,Adolescents,Expectation,Family Influence,Peer Influence,Predictor Variables,Secondary School Students,Sexuality,Social Environment,Student Attitudes,Violence},
mendeley-groups = {Thesis2},
title = {{Re-evaluating the Role of BLEU in Machine Translation Research}},
year = {2006}
}
@article{SentenceLength,
abstract = {The cognitive load theory recommendations for enhancing the success of teaching are effective up to a certain boundary. The paper is dedicated to finding this boundary line in sentence length for 17-18-year-old students. The students filled in the blanks in 30 cloze tests. The cloze test results were correlated with the percentage of sentences over the boundary line. When the boundary-line was low, the coefficient of correlation increased with the rising of the line and the coefficient began to drop when the boundary line passed 140 characters. This size of the boundary line indicated the sentence length, up to which the taking of the load from the learners' mind was effective. The sentences with 130-50 characters were the most suitable for the students.},
annote = {most counts in characters but understanding gets worst in 130-150 characters which are 15-17 words -> use 15 words as upper boundary

good readers can process more but i make lower bounds for it to be easier},
author = {Mikk, Jaan},
doi = {10.1080/03055690701811164},
file = {:C\:/Users/Allgemein/Bachelor/7.Semester/Thesis/paper/Mikk_2008_sentence_length.pdf:pdf},
issn = {03055698},
journal = {Educational Studies},
keywords = {Cloze test,Cognitive load theory,Sentence comprehension,Textbooks,Upper secondary school},
mendeley-groups = {Thesis2},
number = {2},
pages = {119--127},
title = {{Sentence length for revealing the cognitive load reversal effect in text comprehension}},
volume = {34},
year = {2008}
}
@inproceedings{BetterTrans,
abstract = {Many machine translation evaluation metrics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment. It has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation. However, to date there has been no unambiguous report that these new metrics can improve a state-of-the-art machine translation system over its BLEU-tuned baseline. In this paper, we demonstrate that tuning Joshua, a hierarchical phrase-based statistical machine translation system, with the TESLA metrics results in significantly better human-judged translation quality than the BLEU-tuned baseline. TESLA-M in particular is simple and performs well in practice on large datasets. We release all our implementation under an open source license. It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems. {\textcopyright} 2011 Association for Computational Linguistics.},
annote = {comparison of metrics with machine translation and improvement of MT
mainly with training a MT engine
a lot of mre sources!
BLEU: simple, no training, no language specific reurces, tendency to favour short translation despite penality
TER: counts necessary edits for translation reference, simple, no language specific resources, similar to human inutuition
TESLA-M: matches bags of n-grams but with weighs, language spezific, very good for into english
TESLA-F: similar to tesla M, not so good out of english

Tesla outperforms other two consistently
teslas similar as well as Ter and Bleu},
author = {Liu, Chang and Dahlmeier, Daniel and Ng, Hwee Tou},
booktitle = {EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Dahlmeier, Ng - 2011 - Better evaluation metrics lead to better machine translation.pdf:pdf},
isbn = {1937284115},
mendeley-groups = {Thesis2},
pages = {375--384},
title = {{Better evaluation metrics lead to better machine translation}},
year = {2011}
}

@article{Clarity,
abstract = {The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to "the" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SacreBLEU, to facilitate this.},
annote = {variation in bleu scores
comparison betweenstudies dificult
mention not comparison between german and czech because different translation sets},
archivePrefix = {arXiv},
arxivId = {1804.08771},
author = {Post, Matt},
doi = {10.18653/v1/w18-6319},
eprint = {1804.08771},
file = {:C\:/Users/Allgemein/Bachelor/7.Semester/Thesis/paper/1804.08771.pdf:pdf},
mendeley-groups = {Thesis2},
pages = {186--191},
title = {{A Call for Clarity in Reporting BLEU Scores}},
year = {2018}
}

@article{TER,
abstract = {We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU-even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as-or better than-a second human judgment does. {\textcopyright} 2006 The Association for Machine Translation in the Americas.},
annote = {TER explained
hter:edit of the mt we look at. has to be changed if new trans is significantly different},
author = {Snover, Matthew and Dorr, Bonnie and Schwartz, Richard and Micciulla, Linnea and Makhoul, John},
file = {:C\:/Users/Allgemein/Bachelor/7.Semester/Thesis/paper/2006.amta-papers.25.pdf:pdf},
journal = {AMTA 2006 - Proceedings of the 7th Conference of the Association for Machine Translation of the Americas: Visions for the Future of Machine Translation},
mendeley-groups = {Thesis2},
number = {August},
pages = {223--231},
title = {{A study of translation edit rate with targeted human annotation}},
year = {2006}
}

@article{NIST,
abstract = {Abstract Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R&D. Unfortunately, evaluation has not been a very powerful tool in machine translation (MT) research because it requires human judgments and is thus ... \n},
annote = {NIST:Reference translation, scoring how many words occure in the MT and reference
case insensitive
different value for informaton gain
number of reference only moderate iprovement},
author = {Doddington, George},
doi = {10.3115/1289189.1289273},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doddington - 2002 - Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.pdf:pdf},
keywords = {rc22176},
mendeley-groups = {Thesis2},
pages = {138},
title = {{Automatic evaluation of machine translation quality using n-gram co-occurrence statistics}},
year = {2002}
}

@article{METEOR,
abstract = {The Meteor Automatic Metric for Machine Translation evaluation, originally developed and released in 2004, was designed with the explicit goal of producing sentence-level scores which correlate well with human judgments of translation quality. Several key design decisionswere incorporated into Meteor in support of this goal. In contrast with IBM's Bleu, which uses only precision-based features, Meteor uses and emphasizes recall in addition to precision, a property that has been confirmed by several metrics as being critical for high correlation with human judgments. Meteor also addresses the problem of reference translation variability by utilizing flexible word matching, allowing for morphological variants and synonyms to be taken into account as legitimate correspondences. Furthermore, the feature ingredients within Meteor are parameterized, allowing for the tuning of the metric's free parameters in search of values that result in optimal correlation with human judgments. Optimal parameters can be separately tuned for different types of human judgments and for different languages. We discuss the initial design of the Meteor metric, subsequent improvements, and performance in several independent evaluations in recent years. {\textcopyright} Springer Science+Business Media B.V. 2009.},
annote = {Meteor: references; lexical similarity; match synonyms and morphological variabnts},
author = {Lavie, Alon and Denkowski, Michael J.},
doi = {10.1007/s10590-009-9059-4},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavie, Denkowski - 2009 - The METEOR metric for automatic evaluation of Machine Translation.pdf:pdf},
issn = {09226567},
journal = {Machine Translation},
keywords = {Automatic metrics,MT evaluation,Machine Translation},
mendeley-groups = {Thesis2},
number = {2-3},
pages = {105--115},
title = {{The METEOR metric for automatic evaluation of Machine Translation}},
volume = {23},
year = {2009}
}

@article{TERp,
abstract = {This paper describes a new evaluation metric, TER-Plus (TERp) for automatic evaluation of machine translation (MT). TERp is an extension of Translation Edit Rate (TER). It builds on the success of TER as an evaluation metric and alignment tool and addresses several of its weaknesses through the use of paraphrases, stemming, synonyms, as well as edit costs that can be automatically optimized to correlate better with various types of human judgments. We present a correlation study comparing TERp to BLEU, METEOR and TER, and illustrate that TERp can better evaluate translation adequacy. {\textcopyright} Springer Science+Business Media B.V. 2009.},
annote = {Work on TER:
with reference (like ter)
considers synonyms and same stem
hes diferentiated cost for diffrent edits},
author = {Snover, Matthew G. and Madnani, Nitin and Dorr, Bonnie and Schwartz, Richard},
doi = {10.1007/s10590-009-9062-9},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Snover et al. - 2009 - TER-Plus Paraphrase, semantic, and alignment enhancements to translation Edit Rate.pdf:pdf},
isbn = {1059000990629},
issn = {09226567},
journal = {Machine Translation},
keywords = {Alignment,Machine translation evaluation,Paraphrasing},
mendeley-groups = {Thesis2},
number = {2-3},
pages = {117--127},
title = {{TER-Plus: Paraphrase, semantic, and alignment enhancements to translation Edit Rate}},
volume = {23},
year = {2009}
}

@article{TER,
abstract = {We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU-even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as-or better than-a second human judgment does. {\textcopyright} 2006 The Association for Machine Translation in the Americas.},
annote = {TER explained
hter:edit of the mt we look at. has to be changed if new trans is significantly different},
author = {Snover, Matthew and Dorr, Bonnie and Schwartz, Richard and Micciulla, Linnea and Makhoul, John},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Snover et al. - 2006 - A study of translation edit rate with targeted human annotation.pdf:pdf},
journal = {AMTA 2006 - Proceedings of the 7th Conference of the Association for Machine Translation of the Americas: Visions for the Future of Machine Translation},
mendeley-groups = {Thesis2},
number = {August},
pages = {223--231},
title = {{A study of translation edit rate with targeted human annotation}},
year = {2006}
}

@inproceedings{recall,
  title={The significance of recall in automatic metrics for MT evaluation},
  author={Lavie, Alon and Sagae, Kenji and Jayaraman, Shyamsundar},
  booktitle={Conference of the Association for Machine Translation in the Americas},
  pages={134--143},
  year={2004},
  organization={Springer}
}






