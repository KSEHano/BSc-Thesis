@inproceedings{BLEU,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@book{book,
abstract = {This is the first volume that brings together research and practice from academic and industry settings and a combination of human and machine translation evaluation. Its comprehensive collection of papers by leading experts in human and machine translation quality and evaluation who situate current developments and chart future trends fills a clear gap in the literature. This is critical to the successful integration of translation technologies in the industry today, where the lines between human and machine are becoming increasingly blurred by technology: this affects the whole translation landscape, from students and trainers to project managers and professionals, including in-house and freelance translators, as well as, of course, translation scholars and researchers. The editors have broad experience in translation quality evaluation research, including investigations into professional practice with qualitative and quantitative studies, and the contributors are leading experts in their respective fields, providing a unique set of complementary perspectives on human and machine translation quality and evaluation, combining theoretical and applied approaches.},
annote = {Part I: 1 p.20-43 Introduction
Part II und III generell sehr interesant
Part III Specia: several sources to look at for Quest},
author = {Moorkens, Joss and Sheila, Castilho and Gaspari, Federico; and Doherty, Stephen},
booktitle = {Machine Translation},
doi = {10.1007/s10590-019-09241-w},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moorkens et al. - 2018 - Translation quality assessment from principles to practice.pdf:pdf},
issn = {0922-6567},
mendeley-groups = {Thesis,Thesis2},
title = {{Translation quality assessment: from principles to practice}},
url = {http://www.springer.com/series/15798},
year = {2018}
}

@inproceedings{Re-evaluating,
  title={Re-evaluating the role of BLEU in machine translation research},
  author={Callison-Burch, Chris and Osborne, Miles and Koehn, Philipp},
  booktitle={11th Conference of the European Chapter of the Association for Computational Linguistics},
  year={2006}
}

@article{SentenceLength,
abstract = {The cognitive load theory recommendations for enhancing the success of teaching are effective up to a certain boundary. The paper is dedicated to finding this boundary line in sentence length for 17-18-year-old students. The students filled in the blanks in 30 cloze tests. The cloze test results were correlated with the percentage of sentences over the boundary line. When the boundary-line was low, the coefficient of correlation increased with the rising of the line and the coefficient began to drop when the boundary line passed 140 characters. This size of the boundary line indicated the sentence length, up to which the taking of the load from the learners' mind was effective. The sentences with 130-50 characters were the most suitable for the students.},
author = {Mikk, Jaan},
doi = {10.1080/03055690701811164},
file = {:C\:/Users/Allgemein/Bachelor/7.Semester/Thesis/paper/Mikk_2008_sentence_length.pdf:pdf},
issn = {03055698},
journal = {Educational Studies},
keywords = {Cloze test,Cognitive load theory,Sentence comprehension,Textbooks,Upper secondary school},
mendeley-groups = {Thesis2},
number = {2},
pages = {119--127},
title = {{Sentence length for revealing the cognitive load reversal effect in text comprehension}},
volume = {34},
year = {2008}
}
@inproceedings{BetterTrans,
abstract = {Many machine translation evaluation metrics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment. It has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation. However, to date there has been no unambiguous report that these new metrics can improve a state-of-the-art machine translation system over its BLEU-tuned baseline. In this paper, we demonstrate that tuning Joshua, a hierarchical phrase-based statistical machine translation system, with the TESLA metrics results in significantly better human-judged translation quality than the BLEU-tuned baseline. TESLA-M in particular is simple and performs well in practice on large datasets. We release all our implementation under an open source license. It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems. {\textcopyright} 2011 Association for Computational Linguistics.},
annote = {comparison of metrics with machine translation and improvement of MT
mainly with training a MT engine
a lot of mre sources!
BLEU: simple, no training, no language specific reurces, tendency to favour short translation despite penality
TER: counts necessary edits for translation reference, simple, no language specific resources, similar to human inutuition
TESLA-M: matches bags of n-grams but with weighs, language spezific, very good for into english
TESLA-F: similar to tesla M, not so good out of english

Tesla outperforms other two consistently
teslas similar as well as Ter and Bleu},
author = {Liu, Chang and Dahlmeier, Daniel and Ng, Hwee Tou},
booktitle = {EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
file = {:C\:/Users/Allgemein/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Dahlmeier, Ng - 2011 - Better evaluation metrics lead to better machine translation.pdf:pdf},
isbn = {1937284115},
mendeley-groups = {Thesis2},
pages = {375--384},
title = {{Better evaluation metrics lead to better machine translation}},
year = {2011}
}

@article{Clarity,
  title={A call for clarity in reporting BLEU scores},
  author={Post, Matt},
  journal={arXiv preprint arXiv:1804.08771},
  year={2018}
}

@inproceedings{TER,
  title={A study of translation edit rate with targeted human annotation},
  author={Snover, Matthew and Dorr, Bonnie and Schwartz, Richard and Micciulla, Linnea and Makhoul, John},
  booktitle={Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers},
  pages={223--231},
  year={2006}
}

@inproceedings{NIST,
  title={Automatic evaluation of machine translation quality using n-gram co-occurrence statistics},
  author={Doddington, George},
  booktitle={Proceedings of the second international conference on Human Language Technology Research},
  pages={138--145},
  year={2002}
}

@article{METEOR,
abstract = {The Meteor Automatic Metric for Machine Translation evaluation, originally developed and released in 2004, was designed with the explicit goal of producing sentence-level scores which correlate well with human judgments of translation quality. Several key design decisionswere incorporated into Meteor in support of this goal. In contrast with IBM's Bleu, which uses only precision-based features, Meteor uses and emphasizes recall in addition to precision, a property that has been confirmed by several metrics as being critical for high correlation with human judgments. Meteor also addresses the problem of reference translation variability by utilizing flexible word matching, allowing for morphological variants and synonyms to be taken into account as legitimate correspondences. Furthermore, the feature ingredients within Meteor are parameterized, allowing for the tuning of the metric's free parameters in search of values that result in optimal correlation with human judgments. Optimal parameters can be separately tuned for different types of human judgments and for different languages. We discuss the initial design of the Meteor metric, subsequent improvements, and performance in several independent evaluations in recent years. {\textcopyright} Springer Science+Business Media B.V. 2009.},
annote = {Meteor: references; lexical similarity; match synonyms and morphological variabnts},
author = {Lavie, Alon and Denkowski, Michael J.},
doi = {10.1007/s10590-009-9059-4},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavie, Denkowski - 2009 - The METEOR metric for automatic evaluation of Machine Translation.pdf:pdf},
issn = {09226567},
journal = {Machine Translation},
keywords = {Automatic metrics,MT evaluation,Machine Translation},
mendeley-groups = {Thesis2},
number = {2-3},
pages = {105--115},
title = {{The METEOR metric for automatic evaluation of Machine Translation}},
volume = {23},
year = {2009}
}

@article{TERp,
  title={Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate},
  author={Snover, Matthew G and Madnani, Nitin and Dorr, Bonnie and Schwartz, Richard},
  journal={Machine Translation},
  volume={23},
  number={2},
  pages={117--127},
  year={2009},
  publisher={Springer}
}

@article{TER,
abstract = {We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU-even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as-or better than-a second human judgment does. {\textcopyright} 2006 The Association for Machine Translation in the Americas.},
annote = {TER explained
hter:edit of the mt we look at. has to be changed if new trans is significantly different},
author = {Snover, Matthew and Dorr, Bonnie and Schwartz, Richard and Micciulla, Linnea and Makhoul, John},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Snover et al. - 2006 - A study of translation edit rate with targeted human annotation.pdf:pdf},
journal = {AMTA 2006 - Proceedings of the 7th Conference of the Association for Machine Translation of the Americas: Visions for the Future of Machine Translation},
mendeley-groups = {Thesis2},
number = {August},
pages = {223--231},
title = {{A study of translation edit rate with targeted human annotation}},
year = {2006}
}

@inproceedings{recall,
  title={The significance of recall in automatic metrics for MT evaluation},
  author={Lavie, Alon and Sagae, Kenji and Jayaraman, Shyamsundar},
  booktitle={Conference of the Association for Machine Translation in the Americas},
  pages={134--143},
  year={2004},
  organization={Springer}
}

@article{Post,
abstract = {Abstract This paper presents a study conducted in collaboration with Swiss Post's Language Service that aims to compare the performance of a generic neural machine translation system (DeepL) and a customised statistical machine translation system (Microsoft Translator Hub, MTH) in terms of post-editing effort and quality of the final translation for the language direction German-to-French. The results for automatic and human evaluations show that DeepL is overall better than MTH, but its quality is underestimated by the BLEU score.},
annote = {NMT (deepL) compared to SMT
das ist {\"{a}}hnlich zu dem was ich machen will, nur, dass ich die systme nicht selbs trainiere und die Systeme pre-editing von menschen vergleichen lasse},
author = {Volkart, Lise and Bouillon, Pierrette and Girletti, Sabrina},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Volkart, Bouillon, Girletti - 2018 - Statistical vs. Neural Machine Translation A Comparison of MTH and DeepL at Swiss Post's Language S.pdf:pdf},
journal = {Proceedings of the 40th Conference Translating and the Computer},
mendeley-groups = {Thesis2},
pages = {145--150},
title = {{Statistical vs. Neural Machine Translation: A Comparison of MTH and DeepL at Swiss Post's Language Service}},
url = {https://www.matecat.com},
year = {2018}
}

@article{legal,
abstract = {This paper presents work on the evaluation of online available machine translation (MT) service, i.e. Google Translate, for English-Croatian language pair in the domain of legislation. The total set of 200 sentences, for which three reference translations are provided, is divided into short and long sentences. Human evaluation is performed by native speakers, using the criteria of adequacy and fluency. For measuring the reliability of agreement among raters, Fleiss' kappa metric is used. Human evaluation is enriched by error analysis, in order to examine the influence of error types on fluency and adequacy, and to use it in further research. Translation errors are divided into several categories: non-translated words, word omissions, unnecessarily translated words, morphological errors, lexical errors, syntactic errors and incorrect punctuation. The automatic evaluation metric BLEU is calculated with regard to a single and multiple reference translations. System level Pearson's correlation between BLEU scores based on a single and multiple reference translations is given, as well as correlation between short and long sentences BLEU scores, and correlation between the criteria of fluency and adequacy and each error category.},
annote = {BLEU for translation evaluation

Bleu score is better with 2 or even 3 references},
author = {Seljan, Sanja and Vicic, Tomislav and Brkic, Marija},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seljan, Vicic, Brkic - 2012 - BLEU evaluation of machine-translated english-croatian legislation.pdf:pdf},
isbn = {9782951740877},
journal = {Proceedings of the 8th International Conference on Language Resources and Evaluation, LREC 2012},
keywords = {BLEU metric,English-Croatian legislation,Human evaluation},
mendeley-groups = {Thesis2},
pages = {2143--2148},
title = {{BLEU evaluation of machine-translated english-croatian legislation}},
year = {2012}
}
@misc{Google_NMT,
	title = {Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System},
	url = {http://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html},
	abstract = {Posted by Mike Schuster (Google Brain Team), Melvin Johnson (Google Translate) and Nikhil Thorat (Google Brain Team) In the last 10 years, G...},
	language = {en},
	urldate = {2021-07-28},
	journal = {Google AI Blog},
	file = {Snapshot:C\:\\Users\\Allgemein\\Zotero\\storage\\JIG3DBGU\\zero-shot-translation-with-googles.html:text/html},
}

@misc{diffNMT,
	title = {Neural Machine Translation mit menschlichem Touch},
	url = {https://www.supertext.de/de/unternehmung/machine-translation},
	abstract = {Künstliche Intelligenz. Und menschliche Eloquenz. Was immer Sie brauchen, Supertext liefert die Lösung.},
	language = {de-DE},
	urldate = {2021-07-28},
	journal = {Supertext},
	file = {Snapshot:C\:\\Users\\Allgemein\\Zotero\\storage\\N75JXKE3\\machine-translation.html:text/html},
}le

@misc{SDL_NMT,
	title = {Neural Machine Translation in Trados Studio},
	url = {https://www.trados.com/products/machine-translation/access-nmt-in-studio.html},
	abstract = {Neural Machine Translation is the perfect solution for translators looking to use the latest in neural machine translation to automatically translate content.},
	language = {en},
	urldate = {2021-07-28},
	journal = {SDL},
	file = {Snapshot:C\:\\Users\\Allgemein\\Zotero\\storage\\N3EVSMLX\\access-nmt-in-studio.html:text/html},
}

@misc{DeepL,
	title = {DeepL Translate – Der präziseste {Übersetzer} der Welt},
	url = {https://www.DeepL.com/translator},
	abstract = {Schnell, präzise und sicher: Erfahren Sie, warum Millionen von Menschen täglich DeepL nutzen. Übersetzen Sie Texte und komplette Dateien im Handumdrehen. Derzeit werden die Sprachen Bulgarisch, Chinesisch, Dänisch, Deutsch, Niederländisch, Englisch, Estnisch, Finnisch, Französisch, Griechisch, Italienisch, Japanisch, Lettisch, Litauisch, Polnisch, Portugiesisch, Rumänisch, Russisch, Schwedisch, Slowakisch, Slowenisch, Spanisch, Tschechisch und Ungarisch unterstützt.},
	language = {de},
	urldate = {2021-07-28},
	file = {Snapshot:C\:\\Users\\Allgemein\\Zotero\\storage\\CJVIYWVJ\\translator.html:text/html},
}

@misc{Google,
	title = {Google Translate},
	url = {https://translate.google.com/},
	urldate = {2021-07-28},
	file = {Google Übersetzer:C\:\\Users\\Allgemein\\Zotero\\storage\\5EMKS2QR\\translate.google.com.html:text/html},
}
@misc{Bing_NMT,
	title = {Translator Text API - Microsoft Translator for Business},
	url = {https://web.archive.org/web/20190902184414/https://www.microsoft.com/en-us/translator/business/translator-api/},
	urldate = {2021-07-28},
	month = sep,
	year = {2019},
}

@article{QE,
abstract = {Most evaluation metrics for machine translation (MT) require reference translations for each sentence in order to produce a score reflecting certain aspects of its quality. The de facto metrics, BLEU and NIST, are known to have good correlation with human evaluation at the corpus level, but this is not the case at the segment level. As an attempt to overcome these two limitations, we address the problem of evaluating the quality of MT as a prediction task, where reference-independent features are extracted from the input sentences and their translation, and a quality score is obtained based on models produced from training data.We showthat this approach yields better correlation with human evaluation as compared to commonly used metrics, even with models trained on different MT systems, language-pairs and text domains. {\textcopyright} Springer Science+Business Media B.V. 2010.},
annote = {reference independent evaluation (better for segment level)
QE might be better to compre different MT systems
this in the state of research},
author = {Specia, Lucia and Raj, Dhwaj and Turchi, Marco},
doi = {10.1007/s10590-010-9077-2},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Specia, Raj, Turchi - 2010 - Machine translation evaluation versus quality estimation.pdf:pdf},
issn = {09226567},
journal = {Machine Translation},
keywords = {Confidence estimation,Machine translation evaluation,Quality estimation},
mendeley-groups = {Thesis2},
month = {mar},
number = {1},
pages = {39--50},
title = {{Machine translation evaluation versus quality estimation}},
volume = {24},
year = {2010}
}
@misc{Bing,
	title = {Bing {Microsoft} {Translator}},
	url = {https://www.bing.com/Translator},
	urldate = {2021-08-20},
	file = {Bing Microsoft Translator:C\:\\Users\\Allgemein\\Zotero\\storage\\ZQ9VXBNS\\Translator.html:text/html},
}
@article{Quest,
abstract = {We describe QUEST, an open source framework for machine translation quality estimation. The framework allows the ex- traction of several quality indicators from source segments, their translations, exter- nal resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efficacy of fea- tures and algorithms.},
annote = {framework for quality estimation
Quest website
Bl features 17 because good not significant improvement},
author = {Specia, Lucia and Shah, Kashif and De Souza, Jose G. C. and Cohn, Trevor and Kessler, Bruno},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Specia et al. - Unknown - QuEst-A translation quality estimation framework.pdf:pdf},
journal = {ACL (Conference System Demonstrations)},
keywords = {Artificial intelligence,Computer science,Language model,Machine learning,Machine translation,Natural language processing,Parsing,Topic model},
mendeley-groups = {Thesis},
pages = {79--84},
title = {{QuEst - A translation quality estimation framework}},
url = {http://www.dcs.shef.ac.uk/},
year = {2013}
}

@inproceedings{Re-examining,
abstract = {We propose to re-examine the hypothesis that automated metrics developed for MT evaluation can prove useful for paraphrase identification in light of the significant work on the development of new MT metrics over the last 4 years. We show that a meta-classifier trained using nothing but recent MT metrics outperforms all previous paraphrase identification approaches on the Microsoft Research Paraphrase corpus. In addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. Finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community.},
annote = {in this study they show that paraphrasing can be detected with translation metrics

the systematic errors
for introduction

there might be interesting things in the bibliography},
author = {Madnani, Nitin and Tetreault, Joel and Chodorow, Martin},
booktitle = {NAACL HLT 2012 - 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference},
file = {:C\:/Users/kaja/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Madnani, Tetreault, Chodorow - Unknown - Re-examining Machine Translation Metrics for Paraphrase Identification.pdf:pdf},
isbn = {1937284204},
mendeley-groups = {Thesis2},
pages = {182--190},
title = {{Re-examining machine translation metrics for paraphrase identification}},
year = {2012}
}
@misc{nltk,
	title = {nltk.translate.bleu\_score — {NLTK} 3.6 documentation},
	url = {https://www.nltk.org/_modules/nltk/translate/bleu_score.html},
	urldate = {2021-08-03},
	file = {nltk.translate.bleu_score — NLTK 3.6 documentation:C\:\\Users\\Allgemein\\Zotero\\storage\\K5K9IEZU\\bleu_score.html:text/html},
}
@article{Spearman,
  title={Bravais-Pearson and Spearman correlation coefficients: meaning, test of hypothesis and confidence interval},
  author={Artusi, R. and Verderio, P. and Marubini, E.},
  journal={The International journal of biological markers},
  volume={17},
  number={2},
  pages={148--151},
  year={2002},
  publisher={SAGE Publications Sage UK: London, England}
}


